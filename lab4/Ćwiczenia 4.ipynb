{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7f0a76-a7e6-46dd-8f2f-22a84afb16f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./3. Pobierz Dane\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f067b5d4-e84e-49b2-9026-fa9564bc0bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filePath = \"dbfs:/FileStore/tables/Files/201508_station_data.csv\"\n",
    "display(dbutils.fs.ls(\"/FileStore/tables/\"))\n",
    "station_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(filePath)\n",
    "\n",
    "display(station_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ea25a2-f658-410c-819a-cd072abe5d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, regexp_replace, regexp_extract, when, array_contains, explode, avg, count, sum\n",
    "\n",
    "# 1. Nulls - Sprawdzenie brakujących wartości\n",
    "station_data.select([count(when(col(c).isNull(), c)).alias(c) for c in station_data.columns]).show()\n",
    "\n",
    "# 2. fill - Uzupełnienie wartości null domyślną wartością\n",
    "station_data_filled = station_data.fillna({\"dockcount\": \"0\"})\n",
    "\n",
    "# 3. explode - Przykładowe zastosowanie na kolumnie zawierającej listy\n",
    "df_array = station_data.withColumn(\"landmark_array\", expr(\"split(landmark, ' ')\")).select(\"station_id\", explode(col(\"landmark_array\")).alias(\"landmark_word\"))\n",
    "df_array.show()\n",
    "\n",
    "# 4. drop - Usunięcie kolumny \"long\"\n",
    "df_dropped = station_data.drop(\"long\")\n",
    "df_dropped.show()\n",
    "\n",
    "# 5. regexp_replace - Zamiana przecinków na średniki w nazwie stacji\n",
    "df_replaced = station_data.withColumn(\"name\", regexp_replace(col(\"name\"), \",\", \";\"))\n",
    "df_replaced.show()\n",
    "\n",
    "# 6. regexp_extract - Wyodrębnienie tylko pierwszego słowa z kolumny \"name\"\n",
    "df_extracted = station_data.withColumn(\"first_word\", regexp_extract(col(\"name\"), \"(\\\\w+)\", 1))\n",
    "df_extracted.show()\n",
    "\n",
    "# 7. ifnull - Zastąpienie wartości null inną wartością\n",
    "df_ifnull = station_data.withColumn(\"dockcount\", when(col(\"dockcount\").isNull(), 0).otherwise(col(\"dockcount\")))\n",
    "df_ifnull.show()\n",
    "\n",
    "# 8. nullIf - Zastąpienie wartości \"San Jose\" wartością null\n",
    "df_nullif = station_data.withColumn(\"landmark\", expr(\"nullif(landmark, 'San Jose')\"))\n",
    "df_nullif.show()\n",
    "\n",
    "# 9. replace - Zamiana wartości \"San Jose\" na \"SJ\"\n",
    "df_replaced_city = station_data.withColumn(\"landmark\", regexp_replace(col(\"landmark\"), \"San Jose\", \"SJ\"))\n",
    "df_replaced_city.show()\n",
    "\n",
    "# 10. array_contains - Sprawdzenie, czy \"landmark\" zawiera \"San\"\n",
    "df_contains = station_data.withColumn(\"contains_San\", array_contains(expr(\"split(landmark, ' ')\").cast(\"array<string>\"), \"San\"))\n",
    "df_contains.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9352927-00c7-4ad6-82ad-678c6a5d4c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Funkcje agregujące\n",
    "# 1. Średnia liczba stacji (dockcount)\n",
    "station_data.select(avg(\"dockcount\").alias(\"avg_dockcount\")).show()\n",
    "\n",
    "# 2. Liczba stacji w każdym mieście\n",
    "station_data.groupBy(\"landmark\").agg(count(\"station_id\").alias(\"station_count\")).show()\n",
    "\n",
    "# 3. Suma dockcount dla każdego miasta\n",
    "station_data.groupBy(\"landmark\").agg(sum(\"dockcount\").alias(\"total_dockcount\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff40786-d3c2-48f2-bf2f-5c5615b3d8b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-1880178919371002>:12\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    def uppercase_name(name: pd.Series) -> pd.Series:\u001B[0m\n",
       "\u001B[0m                                                     ^\u001B[0m\n",
       "\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected unindent\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-1880178919371002>:12\u001B[0;36m\u001B[0m\n\u001B[0;31m    def uppercase_name(name: pd.Series) -> pd.Series:\u001B[0m\n\u001B[0m                                                     ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected unindent\n",
       "errorSummary": "<span class='ansi-red-fg'>IndentationError</span>: unexpected unindent (<command-1880178919371002>, line 12)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funkcje UDF\n",
    "\n",
    "from pyspark.sql.functions import udf, pandas_udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "import pandas as pd\n",
    "\n",
    "# zwiększanie dockcount o 10%\n",
    "def increase_dockcount(dockcount):\n",
    "    return int(dockcount * 1.1) if dockcount is not None else None\n",
    "\n",
    "    @pandas_udf(StringType())\n",
    "def uppercase_name(name: pd.Series) -> pd.Series:\n",
    "    return name.str.upper()\n",
    "\n",
    "station_data = station_data.withColumn(\"dockcount_adjusted\", increase_dockcount_udf(station_data.dockcount))\n",
    "station_data = station_data.withColumn(\"name_upper\", uppercase_name(station_data.name))\n",
    "\n",
    "display(station_data)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ćwiczenia 4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}